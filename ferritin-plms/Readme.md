# ferritin-plms

## Fully Operational

- Amplify

## WIP

- ESM2
- ESMC
- LigandMPNN



# ferritin-esm

- [esm](https://github.com/evolutionaryscale/esm)
- [cambrian](https://www.evolutionaryscale.ai/blog/esm-cambrian)



# ESMC

## Model Layers

```rust
let model_id = "EvolutionaryScale/esmc-300m-2024-12";
let revision = "main";
let api = Api::new()?;
let repo = api.repo(Repo::with_revision(
    model_id.to_string(),
    RepoType::Model,
    revision.to_string(),
));
let weights_path = repo.get("data/weights/esmc_300m_2024_12_v0.pth")?;
let pth = PthTensors::new(weights_path, None)?;

// print the names
for (name, tensor) in pth.tensor_infos() {
    println!("{}: {:?}", name, tensor);
}
```


```text
embed.weight: TensorInfo { name: "embed.weight", dtype: F32, layout: Layout { shape: [64, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/301", storage_size: 61440 }

transformer.norm.weight: TensorInfo { name: "transformer.norm.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/300", storage_size: 960 }

sequence_head.0.bias: TensorInfo { name: "sequence_head.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/303", storage_size: 960 }
sequence_head.0.weight: TensorInfo { name: "sequence_head.0.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/302", storage_size: 921600 }
sequence_head.2.bias: TensorInfo { name: "sequence_head.2.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/305", storage_size: 960 }
sequence_head.2.weight: TensorInfo { name: "sequence_head.2.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/304", storage_size: 960 }
sequence_head.3.bias: TensorInfo { name: "sequence_head.3.bias", dtype: F32, layout: Layout { shape: [64], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/307", storage_size: 64 }
sequence_head.3.weight: TensorInfo { name: "sequence_head.3.weight", dtype: F32, layout: Layout { shape: [64, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/306", storage_size: 61440 }

transformer.blocks.0.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.0.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/4", storage_size: 960 }
transformer.blocks.0.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.0.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/1", storage_size: 960 }
transformer.blocks.0.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.0.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/0", storage_size: 960 }
transformer.blocks.0.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.0.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/2", storage_size: 2764800 }
transformer.blocks.0.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.0.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/5", storage_size: 921600 }
transformer.blocks.0.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.0.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/3", storage_size: 960 }
transformer.blocks.0.ffn.0.bias: TensorInfo { name: "transformer.blocks.0.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/7", storage_size: 960 }
transformer.blocks.0.ffn.0.weight: TensorInfo { name: "transformer.blocks.0.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/6", storage_size: 960 }
transformer.blocks.0.ffn.1.weight: TensorInfo { name: "transformer.blocks.0.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/8", storage_size: 4915200 }
transformer.blocks.0.ffn.3.weight: TensorInfo { name: "transformer.blocks.0.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/9", storage_size: 2457600 }
```


```text
embed.weight: TensorInfo { name: "embed.weight", dtype: F32, layout: Layout { shape: [64, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/301", storage_size: 61440 }
sequence_head.0.bias: TensorInfo { name: "sequence_head.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/303", storage_size: 960 }
sequence_head.0.weight: TensorInfo { name: "sequence_head.0.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/302", storage_size: 921600 }
sequence_head.2.bias: TensorInfo { name: "sequence_head.2.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/305", storage_size: 960 }
sequence_head.2.weight: TensorInfo { name: "sequence_head.2.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/304", storage_size: 960 }
sequence_head.3.bias: TensorInfo { name: "sequence_head.3.bias", dtype: F32, layout: Layout { shape: [64], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/307", storage_size: 64 }
sequence_head.3.weight: TensorInfo { name: "sequence_head.3.weight", dtype: F32, layout: Layout { shape: [64, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/306", storage_size: 61440 }
transformer.blocks.0.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.0.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/4", storage_size: 960 }
transformer.blocks.0.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.0.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/1", storage_size: 960 }
transformer.blocks.0.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.0.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/0", storage_size: 960 }
transformer.blocks.0.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.0.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/2", storage_size: 2764800 }
transformer.blocks.0.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.0.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/5", storage_size: 921600 }
transformer.blocks.0.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.0.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/3", storage_size: 960 }
transformer.blocks.0.ffn.0.bias: TensorInfo { name: "transformer.blocks.0.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/7", storage_size: 960 }
transformer.blocks.0.ffn.0.weight: TensorInfo { name: "transformer.blocks.0.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/6", storage_size: 960 }
transformer.blocks.0.ffn.1.weight: TensorInfo { name: "transformer.blocks.0.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/8", storage_size: 4915200 }
transformer.blocks.0.ffn.3.weight: TensorInfo { name: "transformer.blocks.0.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/9", storage_size: 2457600 }
transformer.blocks.1.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.1.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/14", storage_size: 960 }
transformer.blocks.1.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.1.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/11", storage_size: 960 }
transformer.blocks.1.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.1.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/10", storage_size: 960 }
transformer.blocks.1.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.1.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/12", storage_size: 2764800 }
transformer.blocks.1.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.1.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/15", storage_size: 921600 }
transformer.blocks.1.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.1.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/13", storage_size: 960 }
transformer.blocks.1.ffn.0.bias: TensorInfo { name: "transformer.blocks.1.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/17", storage_size: 960 }
transformer.blocks.1.ffn.0.weight: TensorInfo { name: "transformer.blocks.1.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/16", storage_size: 960 }
transformer.blocks.1.ffn.1.weight: TensorInfo { name: "transformer.blocks.1.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/18", storage_size: 4915200 }
transformer.blocks.1.ffn.3.weight: TensorInfo { name: "transformer.blocks.1.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/19", storage_size: 2457600 }
transformer.blocks.10.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.10.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/104", storage_size: 960 }
transformer.blocks.10.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.10.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/101", storage_size: 960 }
transformer.blocks.10.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.10.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/100", storage_size: 960 }
transformer.blocks.10.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.10.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/102", storage_size: 2764800 }
transformer.blocks.10.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.10.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/105", storage_size: 921600 }
transformer.blocks.10.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.10.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/103", storage_size: 960 }
transformer.blocks.10.ffn.0.bias: TensorInfo { name: "transformer.blocks.10.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/107", storage_size: 960 }
transformer.blocks.10.ffn.0.weight: TensorInfo { name: "transformer.blocks.10.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/106", storage_size: 960 }
transformer.blocks.10.ffn.1.weight: TensorInfo { name: "transformer.blocks.10.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/108", storage_size: 4915200 }
transformer.blocks.10.ffn.3.weight: TensorInfo { name: "transformer.blocks.10.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/109", storage_size: 2457600 }
transformer.blocks.11.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.11.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/114", storage_size: 960 }
transformer.blocks.11.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.11.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/111", storage_size: 960 }
transformer.blocks.11.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.11.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/110", storage_size: 960 }
transformer.blocks.11.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.11.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/112", storage_size: 2764800 }
transformer.blocks.11.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.11.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/115", storage_size: 921600 }
transformer.blocks.11.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.11.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/113", storage_size: 960 }
transformer.blocks.11.ffn.0.bias: TensorInfo { name: "transformer.blocks.11.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/117", storage_size: 960 }
transformer.blocks.11.ffn.0.weight: TensorInfo { name: "transformer.blocks.11.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/116", storage_size: 960 }
transformer.blocks.11.ffn.1.weight: TensorInfo { name: "transformer.blocks.11.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/118", storage_size: 4915200 }
transformer.blocks.11.ffn.3.weight: TensorInfo { name: "transformer.blocks.11.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/119", storage_size: 2457600 }
transformer.blocks.12.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.12.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/124", storage_size: 960 }
transformer.blocks.12.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.12.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/121", storage_size: 960 }
transformer.blocks.12.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.12.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/120", storage_size: 960 }
transformer.blocks.12.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.12.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/122", storage_size: 2764800 }
transformer.blocks.12.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.12.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/125", storage_size: 921600 }
transformer.blocks.12.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.12.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/123", storage_size: 960 }
transformer.blocks.12.ffn.0.bias: TensorInfo { name: "transformer.blocks.12.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/127", storage_size: 960 }
transformer.blocks.12.ffn.0.weight: TensorInfo { name: "transformer.blocks.12.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/126", storage_size: 960 }
transformer.blocks.12.ffn.1.weight: TensorInfo { name: "transformer.blocks.12.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/128", storage_size: 4915200 }
transformer.blocks.12.ffn.3.weight: TensorInfo { name: "transformer.blocks.12.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/129", storage_size: 2457600 }
transformer.blocks.13.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.13.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/134", storage_size: 960 }
transformer.blocks.13.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.13.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/131", storage_size: 960 }
transformer.blocks.13.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.13.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/130", storage_size: 960 }
transformer.blocks.13.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.13.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/132", storage_size: 2764800 }
transformer.blocks.13.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.13.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/135", storage_size: 921600 }
transformer.blocks.13.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.13.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/133", storage_size: 960 }
transformer.blocks.13.ffn.0.bias: TensorInfo { name: "transformer.blocks.13.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/137", storage_size: 960 }
transformer.blocks.13.ffn.0.weight: TensorInfo { name: "transformer.blocks.13.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/136", storage_size: 960 }
transformer.blocks.13.ffn.1.weight: TensorInfo { name: "transformer.blocks.13.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/138", storage_size: 4915200 }
transformer.blocks.13.ffn.3.weight: TensorInfo { name: "transformer.blocks.13.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/139", storage_size: 2457600 }
transformer.blocks.14.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.14.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/144", storage_size: 960 }
transformer.blocks.14.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.14.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/141", storage_size: 960 }
transformer.blocks.14.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.14.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/140", storage_size: 960 }
transformer.blocks.14.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.14.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/142", storage_size: 2764800 }
transformer.blocks.14.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.14.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/145", storage_size: 921600 }
transformer.blocks.14.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.14.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/143", storage_size: 960 }
transformer.blocks.14.ffn.0.bias: TensorInfo { name: "transformer.blocks.14.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/147", storage_size: 960 }
transformer.blocks.14.ffn.0.weight: TensorInfo { name: "transformer.blocks.14.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/146", storage_size: 960 }
transformer.blocks.14.ffn.1.weight: TensorInfo { name: "transformer.blocks.14.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/148", storage_size: 4915200 }
transformer.blocks.14.ffn.3.weight: TensorInfo { name: "transformer.blocks.14.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/149", storage_size: 2457600 }
transformer.blocks.15.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.15.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/154", storage_size: 960 }
transformer.blocks.15.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.15.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/151", storage_size: 960 }
transformer.blocks.15.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.15.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/150", storage_size: 960 }
transformer.blocks.15.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.15.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/152", storage_size: 2764800 }
transformer.blocks.15.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.15.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/155", storage_size: 921600 }
transformer.blocks.15.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.15.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/153", storage_size: 960 }
transformer.blocks.15.ffn.0.bias: TensorInfo { name: "transformer.blocks.15.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/157", storage_size: 960 }
transformer.blocks.15.ffn.0.weight: TensorInfo { name: "transformer.blocks.15.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/156", storage_size: 960 }
transformer.blocks.15.ffn.1.weight: TensorInfo { name: "transformer.blocks.15.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/158", storage_size: 4915200 }
transformer.blocks.15.ffn.3.weight: TensorInfo { name: "transformer.blocks.15.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/159", storage_size: 2457600 }
transformer.blocks.16.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.16.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/164", storage_size: 960 }
transformer.blocks.16.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.16.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/161", storage_size: 960 }
transformer.blocks.16.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.16.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/160", storage_size: 960 }
transformer.blocks.16.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.16.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/162", storage_size: 2764800 }
transformer.blocks.16.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.16.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/165", storage_size: 921600 }
transformer.blocks.16.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.16.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/163", storage_size: 960 }
transformer.blocks.16.ffn.0.bias: TensorInfo { name: "transformer.blocks.16.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/167", storage_size: 960 }
transformer.blocks.16.ffn.0.weight: TensorInfo { name: "transformer.blocks.16.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/166", storage_size: 960 }
transformer.blocks.16.ffn.1.weight: TensorInfo { name: "transformer.blocks.16.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/168", storage_size: 4915200 }
transformer.blocks.16.ffn.3.weight: TensorInfo { name: "transformer.blocks.16.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/169", storage_size: 2457600 }
transformer.blocks.17.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.17.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/174", storage_size: 960 }
transformer.blocks.17.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.17.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/171", storage_size: 960 }
transformer.blocks.17.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.17.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/170", storage_size: 960 }
transformer.blocks.17.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.17.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/172", storage_size: 2764800 }
transformer.blocks.17.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.17.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/175", storage_size: 921600 }
transformer.blocks.17.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.17.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/173", storage_size: 960 }
transformer.blocks.17.ffn.0.bias: TensorInfo { name: "transformer.blocks.17.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/177", storage_size: 960 }
transformer.blocks.17.ffn.0.weight: TensorInfo { name: "transformer.blocks.17.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/176", storage_size: 960 }
transformer.blocks.17.ffn.1.weight: TensorInfo { name: "transformer.blocks.17.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/178", storage_size: 4915200 }
transformer.blocks.17.ffn.3.weight: TensorInfo { name: "transformer.blocks.17.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/179", storage_size: 2457600 }
transformer.blocks.18.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.18.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/184", storage_size: 960 }
transformer.blocks.18.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.18.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/181", storage_size: 960 }
transformer.blocks.18.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.18.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/180", storage_size: 960 }
transformer.blocks.18.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.18.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/182", storage_size: 2764800 }
transformer.blocks.18.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.18.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/185", storage_size: 921600 }
transformer.blocks.18.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.18.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/183", storage_size: 960 }
transformer.blocks.18.ffn.0.bias: TensorInfo { name: "transformer.blocks.18.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/187", storage_size: 960 }
transformer.blocks.18.ffn.0.weight: TensorInfo { name: "transformer.blocks.18.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/186", storage_size: 960 }
transformer.blocks.18.ffn.1.weight: TensorInfo { name: "transformer.blocks.18.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/188", storage_size: 4915200 }
transformer.blocks.18.ffn.3.weight: TensorInfo { name: "transformer.blocks.18.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/189", storage_size: 2457600 }
transformer.blocks.19.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.19.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/194", storage_size: 960 }
transformer.blocks.19.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.19.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/191", storage_size: 960 }
transformer.blocks.19.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.19.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/190", storage_size: 960 }
transformer.blocks.19.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.19.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/192", storage_size: 2764800 }
transformer.blocks.19.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.19.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/195", storage_size: 921600 }
transformer.blocks.19.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.19.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/193", storage_size: 960 }
transformer.blocks.19.ffn.0.bias: TensorInfo { name: "transformer.blocks.19.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/197", storage_size: 960 }
transformer.blocks.19.ffn.0.weight: TensorInfo { name: "transformer.blocks.19.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/196", storage_size: 960 }
transformer.blocks.19.ffn.1.weight: TensorInfo { name: "transformer.blocks.19.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/198", storage_size: 4915200 }
transformer.blocks.19.ffn.3.weight: TensorInfo { name: "transformer.blocks.19.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/199", storage_size: 2457600 }
transformer.blocks.2.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.2.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/24", storage_size: 960 }
transformer.blocks.2.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.2.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/21", storage_size: 960 }
transformer.blocks.2.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.2.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/20", storage_size: 960 }
transformer.blocks.2.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.2.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/22", storage_size: 2764800 }
transformer.blocks.2.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.2.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/25", storage_size: 921600 }
transformer.blocks.2.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.2.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/23", storage_size: 960 }
transformer.blocks.2.ffn.0.bias: TensorInfo { name: "transformer.blocks.2.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/27", storage_size: 960 }
transformer.blocks.2.ffn.0.weight: TensorInfo { name: "transformer.blocks.2.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/26", storage_size: 960 }
transformer.blocks.2.ffn.1.weight: TensorInfo { name: "transformer.blocks.2.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/28", storage_size: 4915200 }
transformer.blocks.2.ffn.3.weight: TensorInfo { name: "transformer.blocks.2.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/29", storage_size: 2457600 }
transformer.blocks.20.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.20.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/204", storage_size: 960 }
transformer.blocks.20.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.20.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/201", storage_size: 960 }
transformer.blocks.20.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.20.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/200", storage_size: 960 }
transformer.blocks.20.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.20.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/202", storage_size: 2764800 }
transformer.blocks.20.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.20.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/205", storage_size: 921600 }
transformer.blocks.20.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.20.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/203", storage_size: 960 }
transformer.blocks.20.ffn.0.bias: TensorInfo { name: "transformer.blocks.20.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/207", storage_size: 960 }
transformer.blocks.20.ffn.0.weight: TensorInfo { name: "transformer.blocks.20.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/206", storage_size: 960 }
transformer.blocks.20.ffn.1.weight: TensorInfo { name: "transformer.blocks.20.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/208", storage_size: 4915200 }
transformer.blocks.20.ffn.3.weight: TensorInfo { name: "transformer.blocks.20.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/209", storage_size: 2457600 }
transformer.blocks.21.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.21.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/214", storage_size: 960 }
transformer.blocks.21.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.21.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/211", storage_size: 960 }
transformer.blocks.21.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.21.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/210", storage_size: 960 }
transformer.blocks.21.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.21.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/212", storage_size: 2764800 }
transformer.blocks.21.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.21.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/215", storage_size: 921600 }
transformer.blocks.21.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.21.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/213", storage_size: 960 }
transformer.blocks.21.ffn.0.bias: TensorInfo { name: "transformer.blocks.21.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/217", storage_size: 960 }
transformer.blocks.21.ffn.0.weight: TensorInfo { name: "transformer.blocks.21.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/216", storage_size: 960 }
transformer.blocks.21.ffn.1.weight: TensorInfo { name: "transformer.blocks.21.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/218", storage_size: 4915200 }
transformer.blocks.21.ffn.3.weight: TensorInfo { name: "transformer.blocks.21.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/219", storage_size: 2457600 }
transformer.blocks.22.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.22.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/224", storage_size: 960 }
transformer.blocks.22.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.22.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/221", storage_size: 960 }
transformer.blocks.22.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.22.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/220", storage_size: 960 }
transformer.blocks.22.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.22.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/222", storage_size: 2764800 }
transformer.blocks.22.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.22.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/225", storage_size: 921600 }
transformer.blocks.22.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.22.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/223", storage_size: 960 }
transformer.blocks.22.ffn.0.bias: TensorInfo { name: "transformer.blocks.22.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/227", storage_size: 960 }
transformer.blocks.22.ffn.0.weight: TensorInfo { name: "transformer.blocks.22.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/226", storage_size: 960 }
transformer.blocks.22.ffn.1.weight: TensorInfo { name: "transformer.blocks.22.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/228", storage_size: 4915200 }
transformer.blocks.22.ffn.3.weight: TensorInfo { name: "transformer.blocks.22.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/229", storage_size: 2457600 }
transformer.blocks.23.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.23.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/234", storage_size: 960 }
transformer.blocks.23.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.23.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/231", storage_size: 960 }
transformer.blocks.23.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.23.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/230", storage_size: 960 }
transformer.blocks.23.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.23.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/232", storage_size: 2764800 }
transformer.blocks.23.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.23.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/235", storage_size: 921600 }
transformer.blocks.23.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.23.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/233", storage_size: 960 }
transformer.blocks.23.ffn.0.bias: TensorInfo { name: "transformer.blocks.23.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/237", storage_size: 960 }
transformer.blocks.23.ffn.0.weight: TensorInfo { name: "transformer.blocks.23.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/236", storage_size: 960 }
transformer.blocks.23.ffn.1.weight: TensorInfo { name: "transformer.blocks.23.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/238", storage_size: 4915200 }
transformer.blocks.23.ffn.3.weight: TensorInfo { name: "transformer.blocks.23.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/239", storage_size: 2457600 }
transformer.blocks.24.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.24.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/244", storage_size: 960 }
transformer.blocks.24.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.24.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/241", storage_size: 960 }
transformer.blocks.24.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.24.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/240", storage_size: 960 }
transformer.blocks.24.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.24.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/242", storage_size: 2764800 }
transformer.blocks.24.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.24.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/245", storage_size: 921600 }
transformer.blocks.24.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.24.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/243", storage_size: 960 }
transformer.blocks.24.ffn.0.bias: TensorInfo { name: "transformer.blocks.24.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/247", storage_size: 960 }
transformer.blocks.24.ffn.0.weight: TensorInfo { name: "transformer.blocks.24.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/246", storage_size: 960 }
transformer.blocks.24.ffn.1.weight: TensorInfo { name: "transformer.blocks.24.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/248", storage_size: 4915200 }
transformer.blocks.24.ffn.3.weight: TensorInfo { name: "transformer.blocks.24.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/249", storage_size: 2457600 }
transformer.blocks.25.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.25.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/254", storage_size: 960 }
transformer.blocks.25.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.25.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/251", storage_size: 960 }
transformer.blocks.25.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.25.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/250", storage_size: 960 }
transformer.blocks.25.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.25.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/252", storage_size: 2764800 }
transformer.blocks.25.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.25.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/255", storage_size: 921600 }
transformer.blocks.25.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.25.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/253", storage_size: 960 }
transformer.blocks.25.ffn.0.bias: TensorInfo { name: "transformer.blocks.25.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/257", storage_size: 960 }
transformer.blocks.25.ffn.0.weight: TensorInfo { name: "transformer.blocks.25.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/256", storage_size: 960 }
transformer.blocks.25.ffn.1.weight: TensorInfo { name: "transformer.blocks.25.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/258", storage_size: 4915200 }
transformer.blocks.25.ffn.3.weight: TensorInfo { name: "transformer.blocks.25.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/259", storage_size: 2457600 }
transformer.blocks.26.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.26.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/264", storage_size: 960 }
transformer.blocks.26.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.26.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/261", storage_size: 960 }
transformer.blocks.26.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.26.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/260", storage_size: 960 }
transformer.blocks.26.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.26.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/262", storage_size: 2764800 }
transformer.blocks.26.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.26.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/265", storage_size: 921600 }
transformer.blocks.26.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.26.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/263", storage_size: 960 }
transformer.blocks.26.ffn.0.bias: TensorInfo { name: "transformer.blocks.26.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/267", storage_size: 960 }
transformer.blocks.26.ffn.0.weight: TensorInfo { name: "transformer.blocks.26.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/266", storage_size: 960 }
transformer.blocks.26.ffn.1.weight: TensorInfo { name: "transformer.blocks.26.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/268", storage_size: 4915200 }
transformer.blocks.26.ffn.3.weight: TensorInfo { name: "transformer.blocks.26.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/269", storage_size: 2457600 }
transformer.blocks.27.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.27.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/274", storage_size: 960 }
transformer.blocks.27.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.27.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/271", storage_size: 960 }
transformer.blocks.27.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.27.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/270", storage_size: 960 }
transformer.blocks.27.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.27.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/272", storage_size: 2764800 }
transformer.blocks.27.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.27.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/275", storage_size: 921600 }
transformer.blocks.27.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.27.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/273", storage_size: 960 }
transformer.blocks.27.ffn.0.bias: TensorInfo { name: "transformer.blocks.27.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/277", storage_size: 960 }
transformer.blocks.27.ffn.0.weight: TensorInfo { name: "transformer.blocks.27.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/276", storage_size: 960 }
transformer.blocks.27.ffn.1.weight: TensorInfo { name: "transformer.blocks.27.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/278", storage_size: 4915200 }
transformer.blocks.27.ffn.3.weight: TensorInfo { name: "transformer.blocks.27.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/279", storage_size: 2457600 }
transformer.blocks.28.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.28.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/284", storage_size: 960 }
transformer.blocks.28.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.28.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/281", storage_size: 960 }
transformer.blocks.28.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.28.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/280", storage_size: 960 }
transformer.blocks.28.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.28.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/282", storage_size: 2764800 }
transformer.blocks.28.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.28.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/285", storage_size: 921600 }
transformer.blocks.28.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.28.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/283", storage_size: 960 }
transformer.blocks.28.ffn.0.bias: TensorInfo { name: "transformer.blocks.28.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/287", storage_size: 960 }
transformer.blocks.28.ffn.0.weight: TensorInfo { name: "transformer.blocks.28.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/286", storage_size: 960 }
transformer.blocks.28.ffn.1.weight: TensorInfo { name: "transformer.blocks.28.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/288", storage_size: 4915200 }
transformer.blocks.28.ffn.3.weight: TensorInfo { name: "transformer.blocks.28.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/289", storage_size: 2457600 }
transformer.blocks.29.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.29.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/294", storage_size: 960 }
transformer.blocks.29.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.29.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/291", storage_size: 960 }
transformer.blocks.29.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.29.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/290", storage_size: 960 }
transformer.blocks.29.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.29.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/292", storage_size: 2764800 }
transformer.blocks.29.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.29.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/295", storage_size: 921600 }
transformer.blocks.29.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.29.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/293", storage_size: 960 }
transformer.blocks.29.ffn.0.bias: TensorInfo { name: "transformer.blocks.29.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/297", storage_size: 960 }
transformer.blocks.29.ffn.0.weight: TensorInfo { name: "transformer.blocks.29.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/296", storage_size: 960 }
transformer.blocks.29.ffn.1.weight: TensorInfo { name: "transformer.blocks.29.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/298", storage_size: 4915200 }
transformer.blocks.29.ffn.3.weight: TensorInfo { name: "transformer.blocks.29.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/299", storage_size: 2457600 }
transformer.blocks.3.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.3.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/34", storage_size: 960 }
transformer.blocks.3.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.3.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/31", storage_size: 960 }
transformer.blocks.3.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.3.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/30", storage_size: 960 }
transformer.blocks.3.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.3.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/32", storage_size: 2764800 }
transformer.blocks.3.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.3.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/35", storage_size: 921600 }
transformer.blocks.3.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.3.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/33", storage_size: 960 }
transformer.blocks.3.ffn.0.bias: TensorInfo { name: "transformer.blocks.3.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/37", storage_size: 960 }
transformer.blocks.3.ffn.0.weight: TensorInfo { name: "transformer.blocks.3.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/36", storage_size: 960 }
transformer.blocks.3.ffn.1.weight: TensorInfo { name: "transformer.blocks.3.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/38", storage_size: 4915200 }
transformer.blocks.3.ffn.3.weight: TensorInfo { name: "transformer.blocks.3.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/39", storage_size: 2457600 }
transformer.blocks.4.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.4.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/44", storage_size: 960 }
transformer.blocks.4.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.4.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/41", storage_size: 960 }
transformer.blocks.4.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.4.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/40", storage_size: 960 }
transformer.blocks.4.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.4.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/42", storage_size: 2764800 }
transformer.blocks.4.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.4.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/45", storage_size: 921600 }
transformer.blocks.4.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.4.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/43", storage_size: 960 }
transformer.blocks.4.ffn.0.bias: TensorInfo { name: "transformer.blocks.4.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/47", storage_size: 960 }
transformer.blocks.4.ffn.0.weight: TensorInfo { name: "transformer.blocks.4.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/46", storage_size: 960 }
transformer.blocks.4.ffn.1.weight: TensorInfo { name: "transformer.blocks.4.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/48", storage_size: 4915200 }
transformer.blocks.4.ffn.3.weight: TensorInfo { name: "transformer.blocks.4.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/49", storage_size: 2457600 }
transformer.blocks.5.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.5.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/54", storage_size: 960 }
transformer.blocks.5.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.5.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/51", storage_size: 960 }
transformer.blocks.5.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.5.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/50", storage_size: 960 }
transformer.blocks.5.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.5.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/52", storage_size: 2764800 }
transformer.blocks.5.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.5.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/55", storage_size: 921600 }
transformer.blocks.5.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.5.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/53", storage_size: 960 }
transformer.blocks.5.ffn.0.bias: TensorInfo { name: "transformer.blocks.5.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/57", storage_size: 960 }
transformer.blocks.5.ffn.0.weight: TensorInfo { name: "transformer.blocks.5.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/56", storage_size: 960 }
transformer.blocks.5.ffn.1.weight: TensorInfo { name: "transformer.blocks.5.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/58", storage_size: 4915200 }
transformer.blocks.5.ffn.3.weight: TensorInfo { name: "transformer.blocks.5.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/59", storage_size: 2457600 }
transformer.blocks.6.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.6.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/64", storage_size: 960 }
transformer.blocks.6.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.6.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/61", storage_size: 960 }
transformer.blocks.6.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.6.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/60", storage_size: 960 }
transformer.blocks.6.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.6.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/62", storage_size: 2764800 }
transformer.blocks.6.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.6.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/65", storage_size: 921600 }
transformer.blocks.6.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.6.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/63", storage_size: 960 }
transformer.blocks.6.ffn.0.bias: TensorInfo { name: "transformer.blocks.6.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/67", storage_size: 960 }
transformer.blocks.6.ffn.0.weight: TensorInfo { name: "transformer.blocks.6.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/66", storage_size: 960 }
transformer.blocks.6.ffn.1.weight: TensorInfo { name: "transformer.blocks.6.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/68", storage_size: 4915200 }
transformer.blocks.6.ffn.3.weight: TensorInfo { name: "transformer.blocks.6.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/69", storage_size: 2457600 }
transformer.blocks.7.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.7.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/74", storage_size: 960 }
transformer.blocks.7.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.7.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/71", storage_size: 960 }
transformer.blocks.7.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.7.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/70", storage_size: 960 }
transformer.blocks.7.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.7.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/72", storage_size: 2764800 }
transformer.blocks.7.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.7.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/75", storage_size: 921600 }
transformer.blocks.7.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.7.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/73", storage_size: 960 }
transformer.blocks.7.ffn.0.bias: TensorInfo { name: "transformer.blocks.7.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/77", storage_size: 960 }
transformer.blocks.7.ffn.0.weight: TensorInfo { name: "transformer.blocks.7.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/76", storage_size: 960 }
transformer.blocks.7.ffn.1.weight: TensorInfo { name: "transformer.blocks.7.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/78", storage_size: 4915200 }
transformer.blocks.7.ffn.3.weight: TensorInfo { name: "transformer.blocks.7.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/79", storage_size: 2457600 }
transformer.blocks.8.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.8.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/84", storage_size: 960 }
transformer.blocks.8.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.8.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/81", storage_size: 960 }
transformer.blocks.8.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.8.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/80", storage_size: 960 }
transformer.blocks.8.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.8.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/82", storage_size: 2764800 }
transformer.blocks.8.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.8.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/85", storage_size: 921600 }
transformer.blocks.8.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.8.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/83", storage_size: 960 }
transformer.blocks.8.ffn.0.bias: TensorInfo { name: "transformer.blocks.8.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/87", storage_size: 960 }
transformer.blocks.8.ffn.0.weight: TensorInfo { name: "transformer.blocks.8.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/86", storage_size: 960 }
transformer.blocks.8.ffn.1.weight: TensorInfo { name: "transformer.blocks.8.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/88", storage_size: 4915200 }
transformer.blocks.8.ffn.3.weight: TensorInfo { name: "transformer.blocks.8.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/89", storage_size: 2457600 }
transformer.blocks.9.attn.k_ln.weight: TensorInfo { name: "transformer.blocks.9.attn.k_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/94", storage_size: 960 }
transformer.blocks.9.attn.layernorm_qkv.0.bias: TensorInfo { name: "transformer.blocks.9.attn.layernorm_qkv.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/91", storage_size: 960 }
transformer.blocks.9.attn.layernorm_qkv.0.weight: TensorInfo { name: "transformer.blocks.9.attn.layernorm_qkv.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/90", storage_size: 960 }
transformer.blocks.9.attn.layernorm_qkv.1.weight: TensorInfo { name: "transformer.blocks.9.attn.layernorm_qkv.1.weight", dtype: F32, layout: Layout { shape: [2880, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/92", storage_size: 2764800 }
transformer.blocks.9.attn.out_proj.weight: TensorInfo { name: "transformer.blocks.9.attn.out_proj.weight", dtype: F32, layout: Layout { shape: [960, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/95", storage_size: 921600 }
transformer.blocks.9.attn.q_ln.weight: TensorInfo { name: "transformer.blocks.9.attn.q_ln.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/93", storage_size: 960 }
transformer.blocks.9.ffn.0.bias: TensorInfo { name: "transformer.blocks.9.ffn.0.bias", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/97", storage_size: 960 }
transformer.blocks.9.ffn.0.weight: TensorInfo { name: "transformer.blocks.9.ffn.0.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/96", storage_size: 960 }
transformer.blocks.9.ffn.1.weight: TensorInfo { name: "transformer.blocks.9.ffn.1.weight", dtype: F32, layout: Layout { shape: [5120, 960], stride: [960, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/98", storage_size: 4915200 }
transformer.blocks.9.ffn.3.weight: TensorInfo { name: "transformer.blocks.9.ffn.3.weight", dtype: F32, layout: Layout { shape: [960, 2560], stride: [2560, 1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/99", storage_size: 2457600 }

transformer.norm.weight: TensorInfo { name: "transformer.norm.weight", dtype: F32, layout: Layout { shape: [960], stride: [1], start_offset: 0 }, path: "esmc_300m_v0_fp32/data/300", storage_size: 960 }
```


# ferritin-ligandmpnn

- utilities to convert sequence formats (PDB; mmcif) to ML-ready tensors.
- CLI to handle the above.
