{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d26ae8-c171-4c8f-b8ba-e9ce2722a6d3",
   "metadata": {},
   "source": [
    "# Ferritin-Amplify\n",
    "\n",
    "Using the [evcxr jupyter kernel](https://github.com/evcxr/evcxr) to show some Amplify basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ba4e6b-dbe5-4448-8290-80d3434cd90b",
   "metadata": {},
   "outputs": [],
   "source": [
    ":dep candle-core = \"0.8.1\"\n",
    ":dep ferritin-amplify = { path = \"../../../ferritin-amplify/\" , features = [\"metal\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bfbc2e-8f41-4ade-85be-e8a0760b21c9",
   "metadata": {},
   "source": [
    "### Load The Rust code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35f94d52-5310-426e-a1af-f47d91f867d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use candle_core::Device;\n",
    "use ferritin_amplify::{AMPLIFY, AMPLIFYConfig, ModelOutput};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1442c36-1ce5-4028-816a-af6c4eebf13b",
   "metadata": {},
   "source": [
    "### Load the Amplify Data and the Tokenizer.\n",
    "\n",
    "- `tokenizer` is a rustified version of the AMPLIFY tokenizer deposited on HuggingFace we can use it to encode AA sequences to tensors and decode tensors to an alphabet.\n",
    "- `amplify` in this context is the AMPLIFY model rewritten in Rust/Candle and with the AMPLIFY weights from HuggingFace preloaded and ready to go.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a8fac79-30c5-4f02-9aee-95d88d462508",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal(MetalDevice(DeviceId(1)))\n"
     ]
    }
   ],
   "source": [
    "let device = candle_core::Device::new_metal(0)?;\n",
    "println!(\"{:?}\",device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "559440d5-4bee-4048-99f2-d803bbe361a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "let (tokenizer, amplify) = AMPLIFY::load_from_huggingface(device as candle_core::Device)?;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0ecc9d-b12c-4ece-aa91-e2e9967fa584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// println!(\"Tokenizer, Amplify Model: {:?}, {:?}\", tokenizer, amplify);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb26372-043a-4594-b87b-c29cbf3adffb",
   "metadata": {},
   "source": [
    "### Show Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50ae4cce-5f1a-4e5a-adf3-c620203109d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "unused variable: `scores`",
     "output_type": "error",
     "traceback": [
      "\u001b[91munused variable: `scores`\u001b[0m"
     ]
    },
    {
     "ename": "Error",
     "evalue": "unused variable: `is_causal`",
     "output_type": "error",
     "traceback": [
      "\u001b[91munused variable: `is_causal`\u001b[0m"
     ]
    },
    {
     "ename": "Error",
     "evalue": "unused variable: `config`",
     "output_type": "error",
     "traceback": [
      "\u001b[91munused variable: `config`\u001b[0m"
     ]
    },
    {
     "ename": "Error",
     "evalue": "unused variable: `vb`",
     "output_type": "error",
     "traceback": [
      "\u001b[91munused variable: `vb`\u001b[0m"
     ]
    },
    {
     "ename": "Error",
     "evalue": "unused variable: `ampconfig`",
     "output_type": "error",
     "traceback": [
      "\u001b[91munused variable: `ampconfig`\u001b[0m"
     ]
    },
    {
     "ename": "Error",
     "evalue": "associated function `new` is never used",
     "output_type": "error",
     "traceback": [
      "\u001b[91massociated function `new` is never used\u001b[0m"
     ]
    },
    {
     "ename": "Error",
     "evalue": "fields `pad_token_id`, `mask_token_id`, `bos_token_id`, and `eos_token_id` are never read",
     "output_type": "error",
     "traceback": [
      "\u001b[91mfields `pad_token_id`, `mask_token_id`, `bos_token_id`, and `eos_token_id` are never read\u001b[0m"
     ]
    }
   ],
   "source": [
    "let seq_01 = \"GAGARTSYIIVGAGARTSYIIVGAGARTSYIIVGAGARTSYIIVGAGARTSYIIV\";\n",
    "let seq_01 = \"MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQVECPKAPVEWNNPPSEKGLIVGHFSGIKYKGEKAQASEVDVNKMCCWVSKFKDAMRRYQGIQTCKIPGKVLSDLDAKIKAYNLTVEGVEGFVRYSRVTKQHVAAFLKELRHSKQYENVNLIHYILTDKRVDIQHLEKDLVKDFKALVESAHRMRQGHMINVKYILYQLLKKHGHGPDGPDILTVKTGSKGVLYDDSFRKIYTDLGWKFTPL\";\n",
    "    \n",
    "let encoded = tokenizer.encode(&[seq_01.to_string()], None, true, true);\n",
    "println!(\"{:?}\",encoded);\n",
    "// println!(\"Encoded Aminos: {:?}\", tokenizer.encode(&[sprot_01.to_string()]), Some, true, true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70c8d1fe-e8e1-400f-aceb-490789371cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thread '<unnamed>' panicked at /Users/zcpowers/.cargo/registry/src/index.crates.io-6f17d22bba15001f/onig-6.4.0/src/lib.rs:648:23:\n",
      "Onig: Regex search error: Regex encoding does not match haystack encoding (0x10b258e88, 0x107e29190)\n",
      "stack backtrace:\n",
      "   0: _rust_begin_unwind\n",
      "   1: core::panicking::panic_fmt\n",
      "   2: onig::Regex::search_with_encoding\n",
      "   3: <onig::find::FindMatches as core::iter::traits::iterator::Iterator>::next\n",
      "   4: <&tokenizers::utils::onig::SysRegex as tokenizers::tokenizer::pattern::Pattern>::find_matches\n",
      "   5: tokenizers::tokenizer::normalizer::NormalizedString::split\n",
      "   6: tokenizers::tokenizer::pre_tokenizer::PreTokenizedString::split\n",
      "   7: <tokenizers::pre_tokenizers::PreTokenizerWrapper as tokenizers::tokenizer::PreTokenizer>::pre_tokenize\n",
      "   8: tokenizers::tokenizer::TokenizerImpl<M,N,PT,PP,D>::encode_single_sequence::{{closure}}\n",
      "   9: tokenizers::tokenizer::TokenizerImpl<M,N,PT,PP,D>::encode_single_sequence\n",
      "  10: tokenizers::tokenizer::TokenizerImpl<M,N,PT,PP,D>::encode\n",
      "  11: ferritin_amplify::amplify::tokenizer::ProteinTokenizer::encode\n",
      "  12: std::panic::catch_unwind\n",
      "  13: _run_user_code_5\n",
      "  14: evcxr::runtime::Runtime::run_loop\n",
      "  15: evcxr::runtime::runtime_hook\n",
      "  16: evcxr_jupyter::main\n",
      "note: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.\n"
     ]
    }
   ],
   "source": [
    "let seq_01 = \"MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQVECPKAPVEWNNPPSEKGLIVGHFSGIKYKGEKAQASEVDVNKMCCWVSKFKDAMRRYQGIQTCKIPGKVLSDLDAKIKAYNLTVEGVEGFVRYSRVTKQHVAAFLKELRHSKQYENVNLIHYILTDKRVDIQHLEKDLVKDFKALVESAHRMRQGHMINVKYILYQLLKKHGHGPDGPDILTVKTGSKGVLYDDSFRKIYTDLGWKFTPL\";\n",
    "tokenizer.encode(&[seq_01.to_string()], None, true, true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52981df-cdf8-46ef-ad78-7e85760c2389",
   "metadata": {},
   "outputs": [],
   "source": [
    ":clear"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rust",
   "language": "rust",
   "name": "rust"
  },
  "language_info": {
   "codemirror_mode": "rust",
   "file_extension": ".rs",
   "mimetype": "text/rust",
   "name": "Rust",
   "pygment_lexer": "rust",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
